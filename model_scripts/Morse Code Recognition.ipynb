{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Morse Code Recognition.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UtrjUFktFGOz","executionInfo":{"status":"ok","timestamp":1605123150919,"user_tz":480,"elapsed":2203,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}}},"source":["import os\n","import json\n","import numpy as np\n","import math\n","from skimage.measure import block_reduce\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","from collections import Counter\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"kM5klx2eFMve","executionInfo":{"status":"ok","timestamp":1605123174075,"user_tz":480,"elapsed":25350,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"10991204-e717-4b2f-83c5-85333daa8773","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd \"/gdrive/Shared drives/deep learning\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/Shared drives/deep learning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-R8W0HQtFWxJ","executionInfo":{"status":"ok","timestamp":1605123198949,"user_tz":480,"elapsed":50218,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"951bcc8d-37af-49eb-c738-7758f7e90d21","colab":{"base_uri":"https://localhost:8080/"}},"source":["data_filename = \"data/training_data_labels.json\"\n","with open(data_filename) as f:\n","  data_dict = json.load(f)\n","  data = np.array(data_dict[\"training_data\"])\n","  data = data.reshape(np.append(data.shape, 1))\n","  data = data.reshape(np.append(data.shape, 1))\n","  text_labels = np.array(data_dict[\"training_labels\"])\n","\n","i = 1\n","print(\"Data array shape: \", data.shape, \"\\nFirst {} rows:\".format(i))\n","print(data[:i], end='\\n\\n')\n","\n","print(\"Text labels shape: \", text_labels.shape, \"\\nFirst {} rows:\".format(i))\n","print(text_labels[:i], end='\\n\\n')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Data array shape:  (12237, 6613, 1, 1) \n","First 1 rows:\n","[[[[0.04407315]]\n","\n","  [[0.04305174]]\n","\n","  [[0.05130609]]\n","\n","  ...\n","\n","  [[0.        ]]\n","\n","  [[0.        ]]\n","\n","  [[0.        ]]]]\n","\n","Text labels shape:  (12237,) \n","First 1 rows:\n","['the film won the naacp image award for outstanding']\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bbdeGAs1Gb_s","executionInfo":{"status":"ok","timestamp":1605123204647,"user_tz":480,"elapsed":390,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}}},"source":["max_length = max([len(text_label) for text_label in text_labels])\n","img_width = 6613\n","img_height = 1\n","batch_size = 16"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQQubdkoFfBU","executionInfo":{"status":"ok","timestamp":1605123205528,"user_tz":480,"elapsed":471,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"f3ce9f85-68ed-4fc1-ed3b-719186fec5a4","colab":{"base_uri":"https://localhost:8080/"}},"source":["alphabet = list('abcdefghijklmnopqrstuvwxyz0123456789,.?;:-_()=+@$!&/\\'\\\" ')\n","alphabet_size = len(alphabet)\n","\n","# Define a mapping between characters and indices\n","char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n","int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n","\n","max_text_length = 0\n","for d in text_labels:\n","  l = len(d)\n","  if l > max_text_length:\n","    max_text_length = l\n","\n","num_rows = len(text_labels)\n","labels = np.zeros((num_rows, max_text_length), dtype=np.int8)\n","\n","for i, line in enumerate(text_labels):\n","  # Target text data -> integer encodings\n","  labels[i] = np.array([char_to_int[char] for char in line])\n","\n","print(text_labels[:2])\n","print(labels[:2])\n","#char_to_num = layers.experimental.preprocessing.StringLookup(\n","#    vocabulary=alphabet, num_oov_indices=0, mask_token=None)\n","#\n","## Mapping integers back to original characters\n","#num_to_char = layers.experimental.preprocessing.StringLookup(\n","#    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n","\n","# label = char_to_num(tf.strings.unicode_split(labels[0], input_encoding=\"UTF-8\"))\n","# print(label)\n","# print(labels[0])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['the film won the naacp image award for outstanding'\n"," 'the keystone bridge company, founded in 1865 by an']\n","[[19  7  4 54  5  8 11 12 54 22 14 13 54 19  7  4 54 13  0  0  2 15 54  8\n","  12  0  6  4 54  0 22  0 17  3 54  5 14 17 54 14 20 19 18 19  0 13  3  8\n","  13  6]\n"," [19  7  4 54 10  4 24 18 19 14 13  4 54  1 17  8  3  6  4 54  2 14 12 15\n","   0 13 24 36 54  5 14 20 13  3  4  3 54  8 13 54 27 34 32 31 54  1 24 54\n","   0 13]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EzbLYcbIH4dh","executionInfo":{"status":"ok","timestamp":1605123206627,"user_tz":480,"elapsed":420,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}}},"source":["def split_data(images, labels, train_size=0.9, shuffle=True):\n","    # 1. Get the total size of the dataset\n","    size = len(images)\n","\n","    # 2. Make an indices array and shuffle it, if required\n","    indices = np.arange(size)\n","    if shuffle:\n","        np.random.shuffle(indices)\n","\n","    # 3. Get the size of training samples\n","    train_samples = int(size * train_size)\n","\n","    # 4. Split data into training and validation sets\n","    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n","    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n","    return x_train, x_valid, y_train, y_valid\n","\n","# Splitting data into training and validation sets\n","x_train, x_valid, y_train, y_valid = split_data(data, labels)\n","\n","def encode_single_sample(img, label):\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","\n","    # # 4. Resize to the desired size\n","    img = tf.image.resize(img, [img_height, img_width])\n","\n","    # 5. Transpose the image because we want the time\n","    # dimension to correspond to the width of the image.\n","    img = tf.transpose(img, perm=[1, 0, 2])\n","\n","    # 7. Return a dict as our model is expecting two inputs\n","    return {\"image\": img, \"label\": label}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjhRIhigHtxU","executionInfo":{"status":"ok","timestamp":1605123207417,"user_tz":480,"elapsed":357,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"a067ccdc-6d64-4957-9d92-eec9fe021fee","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(text_labels.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(12237,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sMU4bEC8H8S5","executionInfo":{"status":"ok","timestamp":1605123213860,"user_tz":480,"elapsed":6050,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}}},"source":["train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = (\n","    train_dataset.map(\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    )\n","    .batch(batch_size)\n","    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",")\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n","validation_dataset = (\n","    validation_dataset.map(\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    )\n","    .batch(batch_size)\n","    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"5P_x8uXdYFko","executionInfo":{"status":"ok","timestamp":1605123216697,"user_tz":480,"elapsed":819,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"2260889a-1136-44bf-dbc4-47901e14d955","colab":{"base_uri":"https://localhost:8080/"}},"source":["for batch in train_dataset.take(1):\n","  images = batch[\"image\"]\n","  labels = batch[\"label\"]\n","  print(labels)\n","  break\n","print(text_labels[1])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[19  7  4 18  4 54 19  4 17 12 18 54 13 14 54 11 14 13  6  4 17 54  0 15\n","  15 11 24 54 22  8 19  7 54  0 13 24 54  0  2  2 20 17  0  2 24 54  0 18\n","  54 15]\n"," [ 7  4 54  4  0 17 13  4  3 54  0 54  1 17 14 13 25  4 54 12  4  3  0 11\n","  54  8 13 54 19  7  4 54 31 26 26 26 41 12  4 19  4 17 54 17  4 11  0 24\n","  54 22]\n"," [ 8 19 18 54  7  0  1  8 19  0 19 54  8 18 54 18  0 13  3 54 15  0 19  2\n","   7  4 18 54  0 12 14 13  6 54 17  4  4  5 18 54  0 13  3 54 17 20  1  1\n","  11  4]\n"," [ 8 13 54 27 35 35 30 36 54  7  4 54 22  0 18 54  3  8  0  6 13 14 18  4\n","   3 54  0 18 54  7  8 21 41 15 14 18  8 19  8 21  4 36 54  1 20 19 54  7\n","   0 18]\n"," [19  7  4 54 12 20 13 18 19  4 17 54 20 13  3  4 17 41 28 27 54  5 14 14\n","  19  1  0 11 11 54  2  7  0 12 15  8 14 13 18  7  8 15 54  8 18 54 19  7\n","   4 54]\n"," [19  7  4 54 21  8 11 11  0  6  4 54 14  5 54 20 19  4 54 15  0 17 10 36\n","  54 14 15 15 14 18  8 19  4 54 19  7  4 54 12 14 20 19  7 54 14  5 54 20\n","  19  4]\n"," [11  0 19  4 17 36 54 10  8 13  3  4 17  6  0 17 19  4 13 54 22  0 18 54\n","  15 17 14 21  8  3  4  3 54 19  7 17 14 20  6  7 54  2  4 13 19 17  0 11\n","  54  7]\n"," [ 8 13 54 19  7  4  8 17 54 16 20  0 11  8  5  8  2  0 19  8 14 13 54  7\n","   4  0 19 54  7  4  8 18  4 13 36 54  7 14 14  6 12 14  4  3 36 54 21  0\n","  13 54]\n"," [ 2  4 11 11  7  4 11 12  4 19 54  8 18 54  0 54 17  4  6  8 18 19  4 17\n","   4  3 54 19 17  0  3  4 12  0 17 10 54 14  5 54 10  0 13  4 54  0 13  3\n","  54 12]\n"," [ 0 17  0 20  2 13  4 15  7  8 14  8  3  4 18 54  8 18 54  0 54  6  4 13\n","  20 18 54 14  5 54  1 11  0  2 10 54  5 11  8  4 18 54  5 17 14 12 54  2\n","   7  8]\n"," [ 8 19 54  8 18 54 14 13  4 54 14  5 54 19  7  4 54  7  8 18 19 14 17  8\n","   2  0 11 54 15 11  0  2  4 18 54 22  7  4 17  4 54 22  8 11 11 10  0 10\n","  20 19]\n"," [ 7 24 15  4 17  3  4 21  1 14 23 54  9  0 15  0 13 54 43 15 17  4 21  8\n","  14 20 18 11 24 54 10 13 14 22 13 54  0 18 54  7 24 15  4 17 41  3  4 21\n","   1 14]\n"," [19  7  4 54 17  4 21 37 54  3 17 37 54  7  0 22 19 17  4 24 54 22  0 18\n","  54 19  7  4 54 19  7  4 13 41  7  4  0  3 12  0 18 19  4 17 54  0 13  3\n","  54  6]\n"," [ 8 13 54 12 14  3  4 17 13 54 20 18  4 36 54  8 19 54 17  4  5  4 17 18\n","  54 19 14 54 19  7  4 54 15 17  0  2 19  8  2  4 54 14  5 54 18  8 19 19\n","   8 13]\n"," [19  7  4 54  2 14 12 12 20 13  8 19 24 54  8 18 54 11 14  2  0 19  4  3\n","  54  0 19 54 19  7  4 54 18 14 20 19  7  4 17 13 12 14 18 19 54  0 17  4\n","   0 54]\n"," [ 0  2  2 14 17  3  8 13  6 54 19 14 54 18 14 12  4 54 17  4 15 14 17 19\n","  18 36 54  7 14 22  4 21  4 17 36 54  8 19 54  8 18 54 17  4  9  4  2 19\n","   4  3]], shape=(16, 50), dtype=int8)\n","the keystone bridge company, founded in 1865 by an\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7XabLIqwIH9A","executionInfo":{"status":"ok","timestamp":1605123229867,"user_tz":480,"elapsed":642,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}}},"source":["class CTCLayer(layers.Layer):\n","    def __init__(self, name=None):\n","        super().__init__(name=name)\n","        self.loss_fn = keras.backend.ctc_batch_cost\n","\n","    def call(self, y_true, y_pred):\n","        # Compute the training-time loss value and add it\n","        # to the layer using `self.add_loss()`.\n","        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n","        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n","        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n","\n","        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n","        self.add_loss(loss)\n","\n","        # At test time, just return the computed predictions\n","        return y_pred\n","\n","\n","def build_model():\n","    # Inputs to the model\n","    input_img = layers.Input(\n","        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n","    )\n","    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n","\n","    # First conv block\n","    x = layers.Conv2D(\n","        128,\n","        (25, 1),\n","        activation=\"relu\",\n","        kernel_initializer=\"he_normal\",\n","        padding=\"same\",\n","        name=\"Conv1\",\n","    )(input_img)\n","\n","\n","    x = layers.MaxPooling2D((2, 1), name=\"pool1\")(x)\n","    \n","    x = layers.Conv2D(\n","        128,\n","        (25, 1),\n","        activation=\"relu\",\n","        kernel_initializer=\"he_normal\",\n","        padding=\"same\",\n","        name=\"Conv2\",\n","    )(x)\n","    x = layers.MaxPooling2D((2, 1), name=\"pool2\")(x)\n","   \n","    # Second conv block\n","    x = layers.Conv2D(\n","        128,\n","        (25, 1),\n","        activation=\"relu\",\n","        kernel_initializer=\"he_normal\",\n","        padding=\"same\",\n","        name=\"Conv3\",\n","    )(x)\n","    x = layers.MaxPooling2D((2, 1), name=\"pool3\")(x)\n","\n","    # We have used two max pool with pool size and strides 2.\n","    # Hence, downsampled feature maps are 4x smaller. The number of\n","    # filters in the last layer is 64. Reshape accordingly before\n","    # passing the output to the RNN part of the model\n","    new_shape = ((img_width // 8), 1 * 128)\n","    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n","    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n","    x = layers.Dropout(0.2)(x)\n","\n","    # RNNs\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n","    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n","\n","    # Output layer\n","    x = layers.Dense(len(alphabet) + 1, activation=\"softmax\", name=\"dense2\")(x)\n","\n","    # Add CTC layer for calculating CTC loss at each step\n","    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n","\n","    # Define the model\n","    model = keras.models.Model(\n","        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n","    )\n","    # Optimizer\n","    opt = keras.optimizers.Adam()\n","    # Compile the model and return\n","    model.compile(optimizer=opt)\n","    return model"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhxxZdR3NUAo","executionInfo":{"status":"ok","timestamp":1605123231582,"user_tz":480,"elapsed":1590,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"ae82baff-d4f4-490b-acf8-1dd9b33aae28","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = build_model()\n","model.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Model: \"ocr_model_v1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","image (InputLayer)              [(None, 6613, 1, 1)] 0                                            \n","__________________________________________________________________________________________________\n","Conv1 (Conv2D)                  (None, 6613, 1, 128) 3328        image[0][0]                      \n","__________________________________________________________________________________________________\n","pool1 (MaxPooling2D)            (None, 3306, 1, 128) 0           Conv1[0][0]                      \n","__________________________________________________________________________________________________\n","Conv2 (Conv2D)                  (None, 3306, 1, 128) 409728      pool1[0][0]                      \n","__________________________________________________________________________________________________\n","pool2 (MaxPooling2D)            (None, 1653, 1, 128) 0           Conv2[0][0]                      \n","__________________________________________________________________________________________________\n","Conv3 (Conv2D)                  (None, 1653, 1, 128) 409728      pool2[0][0]                      \n","__________________________________________________________________________________________________\n","pool3 (MaxPooling2D)            (None, 826, 1, 128)  0           Conv3[0][0]                      \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 826, 128)     0           pool3[0][0]                      \n","__________________________________________________________________________________________________\n","dense1 (Dense)                  (None, 826, 64)      8256        reshape[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 826, 64)      0           dense1[0][0]                     \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 826, 256)     197632      dropout[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_2 (Bidirectional) (None, 826, 128)     164352      bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","label (InputLayer)              [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","dense2 (Dense)                  (None, 826, 56)      7224        bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","ctc_loss (CTCLayer)             (None, 826, 56)      0           label[0][0]                      \n","                                                                 dense2[0][0]                     \n","==================================================================================================\n","Total params: 1,200,248\n","Trainable params: 1,200,248\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j0l7FojUNXp7","executionInfo":{"status":"ok","timestamp":1605118640072,"user_tz":480,"elapsed":1044361,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}},"outputId":"335183a2-0df8-4f15-ac6c-67243950cf63","colab":{"base_uri":"https://localhost:8080/"}},"source":["epochs = 5\n","early_stopping_patience = 5\n","# Add early stopping\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",")\n","\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=epochs,\n","    callbacks=[early_stopping])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","689/689 [==============================] - 217s 315ms/step - loss: 172.8810 - val_loss: 152.6756\n","Epoch 2/5\n","689/689 [==============================] - 215s 312ms/step - loss: 152.2597 - val_loss: 151.8785\n","Epoch 3/5\n","689/689 [==============================] - 201s 292ms/step - loss: 151.7299 - val_loss: 151.7234\n","Epoch 4/5\n","689/689 [==============================] - 201s 291ms/step - loss: 151.4588 - val_loss: 151.3941\n","Epoch 5/5\n","689/689 [==============================] - 202s 293ms/step - loss: 151.1913 - val_loss: 153.0588\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9VtvQKFKPR2h","executionInfo":{"status":"error","timestamp":1605085619621,"user_tz":480,"elapsed":1905386,"user":{"displayName":"Onur Orhan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggoa3BIxcV7hj2hzLzkIi45SvQ6ex6ZKDtT0l1M=s64","userId":"00047847318172694407"}},"outputId":"13ee6fe7-d1f3-48cb-f171-783bec367f8a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["prediction_model = keras.models.Model(\n","    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",")\n","prediction_model.summary()\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","  input_len = np.ones(pred.shape[0]) * pred.shape[1]\n","  # Use greedy search. For complex tasks, you can use beam search\n","  print(pred)\n","  results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n","      :, :max_length\n","  ]\n","  # Iterate over the results and get back the text\n","  output_text = []\n","  print(results)\n","  for res in results:\n","    output_text.append(''.join([int_to_char[i] for i in res]))\n","  return output_text\n","\n","\n","#  Let's check results on some validation samples\n","for batch in validation_dataset.take(1):\n","  batch_images = batch[\"image\"]\n","  batch_labels = batch[\"label\"]\n","\n","  print(batch_images.shape)\n","  preds = prediction_model.predict(batch_images)\n","  print(preds.shape)\n","\n","  pred_texts = decode_batch_predictions(preds)\n","\n","  orig_texts = []\n","  for s, sentence in enumerate(batch_labels):\n","    orig_texts.append([])\n","    \n","    for c, char in enumerate(sentence):\n","      orig_texts[-1].append(int_to_char[char])\n","\n","  for i in range(len(pred_texts)):\n","    print(\"\\nPrediction:\\t{}\".format(pred_texts[i]))\n","    print(\"Ground truth:\\t{}\".format(orig_texts[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(16, 6613, 1, 1)\n","(16, 1653, 56)\n","[[[5.99068440e-02 2.21103951e-02 2.35201493e-02 ... 9.04412067e-04\n","   9.67717351e-05 1.05868444e-01]\n","  [6.55134767e-02 4.29294771e-03 6.03590440e-03 ... 3.53049632e-04\n","   8.14407424e-04 1.36972070e-01]\n","  [2.51089893e-02 2.30153976e-03 4.55265772e-03 ... 1.80286108e-04\n","   4.46430966e-02 3.05396020e-01]\n","  ...\n","  [1.85406255e-03 4.04833379e-04 7.10422057e-04 ... 2.02795200e-05\n","   5.95310004e-03 9.72938180e-01]\n","  [2.47144792e-03 5.25899639e-04 9.59842466e-04 ... 3.18355851e-05\n","   8.01945571e-03 9.65027511e-01]\n","  [6.73409645e-03 1.36613171e-03 2.62637297e-03 ... 1.61882548e-04\n","   2.14270912e-02 9.10743237e-01]]\n","\n"," [[6.55186176e-02 2.30398104e-02 2.50969958e-02 ... 9.93411755e-04\n","   8.23496230e-05 9.41856876e-02]\n","  [7.18142316e-02 4.63645300e-03 6.68268697e-03 ... 3.68426903e-04\n","   6.94821880e-04 1.21637307e-01]\n","  [2.80850101e-02 2.52690190e-03 5.89228701e-03 ... 1.90200459e-04\n","   3.93353403e-02 3.04469347e-01]\n","  ...\n","  [1.95962191e-03 4.71568550e-04 7.86294346e-04 ... 2.25453459e-05\n","   5.84567664e-03 9.71448362e-01]\n","  [2.43886351e-03 5.62293921e-04 9.73072427e-04 ... 3.11657204e-05\n","   7.52239814e-03 9.65049863e-01]\n","  [6.19853102e-03 1.31481071e-03 2.40295217e-03 ... 1.32004105e-04\n","   1.84674300e-02 9.18152928e-01]]\n","\n"," [[5.99068440e-02 2.21103951e-02 2.35201493e-02 ... 9.04412067e-04\n","   9.67717351e-05 1.05868444e-01]\n","  [6.55134767e-02 4.29294771e-03 6.03590440e-03 ... 3.53049632e-04\n","   8.14407424e-04 1.36972070e-01]\n","  [2.51089893e-02 2.30153976e-03 4.55265772e-03 ... 1.80286108e-04\n","   4.46430966e-02 3.05396020e-01]\n","  ...\n","  [1.85406255e-03 4.04833379e-04 7.10422057e-04 ... 2.02795200e-05\n","   5.95310004e-03 9.72938180e-01]\n","  [2.47144792e-03 5.25899639e-04 9.59842466e-04 ... 3.18355851e-05\n","   8.01945571e-03 9.65027511e-01]\n","  [6.73409645e-03 1.36613171e-03 2.62637297e-03 ... 1.61882548e-04\n","   2.14270912e-02 9.10743237e-01]]\n","\n"," ...\n","\n"," [[5.99068999e-02 2.21104361e-02 2.35201903e-02 ... 9.04412474e-04\n","   9.67716551e-05 1.05868191e-01]\n","  [6.55136630e-02 4.29295609e-03 6.03592861e-03 ... 3.53049341e-04\n","   8.14409403e-04 1.36971876e-01]\n","  [2.51089986e-02 2.30153627e-03 4.55268752e-03 ... 1.80285657e-04\n","   4.46432717e-02 3.05398047e-01]\n","  ...\n","  [1.85406161e-03 4.04834514e-04 7.10423396e-04 ... 2.02795400e-05\n","   5.95310330e-03 9.72938180e-01]\n","  [2.47144559e-03 5.25899930e-04 9.59842466e-04 ... 3.18354942e-05\n","   8.01944826e-03 9.65027511e-01]\n","  [6.73408853e-03 1.36612868e-03 2.62636598e-03 ... 1.61881806e-04\n","   2.14270186e-02 9.10743415e-01]]\n","\n"," [[6.53654635e-02 2.30600853e-02 2.50619389e-02 ... 9.89990309e-04\n","   8.32045334e-05 9.42994654e-02]\n","  [7.16922507e-02 4.63610888e-03 6.69149868e-03 ... 3.68506036e-04\n","   7.08461972e-04 1.22150801e-01]\n","  [2.80821249e-02 2.52889632e-03 5.90177253e-03 ... 1.90841325e-04\n","   3.97710390e-02 3.06432486e-01]\n","  ...\n","  [1.95713458e-03 4.71202715e-04 7.85189972e-04 ... 2.25150961e-05\n","   5.85080311e-03 9.71457779e-01]\n","  [2.43696594e-03 5.61525114e-04 9.72159090e-04 ... 3.11432377e-05\n","   7.52826314e-03 9.65074062e-01]\n","  [6.22036168e-03 1.31871109e-03 2.41117482e-03 ... 1.32926318e-04\n","   1.85206421e-02 9.17965591e-01]]\n","\n"," [[5.99068440e-02 2.21103951e-02 2.35201493e-02 ... 9.04412067e-04\n","   9.67717351e-05 1.05868444e-01]\n","  [6.55134767e-02 4.29294771e-03 6.03590440e-03 ... 3.53049632e-04\n","   8.14407424e-04 1.36972070e-01]\n","  [2.51089893e-02 2.30153976e-03 4.55265772e-03 ... 1.80286108e-04\n","   4.46430966e-02 3.05396020e-01]\n","  ...\n","  [1.85406255e-03 4.04833379e-04 7.10422057e-04 ... 2.02795200e-05\n","   5.95310004e-03 9.72938180e-01]\n","  [2.47144792e-03 5.25899639e-04 9.59842466e-04 ... 3.18355851e-05\n","   8.01945571e-03 9.65027511e-01]\n","  [6.73409645e-03 1.36613171e-03 2.62637297e-03 ... 1.61882548e-04\n","   2.14270912e-02 9.10743237e-01]]]\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","tf.Tensor(\n","[[19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]\n"," [19  7  4 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n","  -1 -1]], shape=(16, 50), dtype=int64)\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-97f66a306458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mpred_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_batch_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0morig_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-97f66a306458>\u001b[0m in \u001b[0;36mdecode_batch_predictions\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-97f66a306458>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutput_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and\n\u001b[1;32m    822\u001b[0m         (g is None or g.building_function)):\n\u001b[0;32m--> 823\u001b[0;31m       raise TypeError(\"Tensor is unhashable. \"\n\u001b[0m\u001b[1;32m    824\u001b[0m                       \"Instead, use tensor.ref() as the key.\")\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Tensor is unhashable. Instead, use tensor.ref() as the key."]}]},{"cell_type":"code","metadata":{"id":"Prj3ssMeoNvl"},"source":[""],"execution_count":null,"outputs":[]}]}