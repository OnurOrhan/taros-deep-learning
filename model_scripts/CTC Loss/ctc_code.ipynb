{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ctc_code.ipynb","provenance":[],"authorship_tag":"ABX9TyM77z5vxlVMOE2CqOROjia0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"PRw9WxsuvSVb","executionInfo":{"status":"ok","timestamp":1605122244411,"user_tz":480,"elapsed":2933,"user":{"displayName":"Ankur Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQyz130P1h59W3Uz_bIGBecHDcUmQHcNH7daPV=s64","userId":"10372974935693472350"}}},"source":["import os\n","import json\n","import numpy as np\n","import math\n","from skimage.measure import block_reduce\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def read_train_data():\n","    # Load the data dictionary with all the arrays\n","    with open(os.path.join(data_dir, data_filename)) as f:\n","        data_dict = json.load(f)\n","\n","    # Load the text labels dictionary\n","    with open(os.path.join(data_dir, labels_filename)) as f:\n","        labels_dict = json.load(f)\n","\n","    # Find the maximum size for any array and any text label\n","    # so that we can create fixed sized numpy arrays\n","    max_data_size = 0\n","    max_text_size = 0\n","    num_rows = 0\n","    for filename, val in data_dict.items():\n","        num_rows += 1\n","\n","        data_size = len(data_dict[filename])\n","        if data_size > max_data_size:\n","            max_data_size = data_size\n","\n","        text_size = len(labels_dict[filename])\n","        if text_size > max_text_size:\n","            max_text_size = text_size\n","\n","    # We will reduce the size of our arrays\n","    # by using the block_reduce function\n","    # and averaging array values in intervals of FILTER_SIZE\n","    new_size = math.ceil(max_data_size / FILTER_SIZE)\n","    # Now we have our fixed size array for our down-sampled data\n","    data = np.zeros((num_rows, new_size))\n","\n","    texts = []\n","    i = 0\n","    for filename, arr in data_dict.items():\n","\n","        #turning all values positive before taking the mean (change by ankur)\n","        arr = np.absolute(arr)\n","\n","        new_arr = block_reduce(np.array(arr), block_size=(FILTER_SIZE,), func=np.mean)\n","        new_arr_size = len(new_arr)\n","        # The array is probably smaller than the maximum allowed length\n","        # So let's set the boundary for that\n","        data[i, :new_arr_size] = new_arr\n","\n","        text = labels_dict[filename]\n","        # We are padding text labels with empty strings if they are short\n","        texts.append(text.ljust(max_text_size).lower())\n","        i += 1\n","\n","    text_data = np.array(texts)\n","    return data, text_data"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLECRqb0vZwW"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","%cd \"gdrive/Shared drives/deep learning\"\n","data_dir = \"data\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqMFgUX7vdI9"},"source":["## ----- READING DOWNSAMPLED DATA ----- ##\n","\n","data_filename = os.path.join(data_dir, \"training_data_labels.json\")\n","with open(data_filename) as f:\n","  data_dict = json.load(f)\n","  data = np.array(data_dict[\"training_data\"])\n","  data = data.reshape(np.append(data.shape, 1))\n","  \n","  text_data = np.array(data_dict[\"training_labels\"])\n","\n","print(\"Data array shape: \", data.shape, \"\\nFirst 2 rows:\")\n","print(data[:2], end='\\n\\n')\n","\n","print(\"Text labels shape: \", text_data.shape, \"\\nFirst 2 rows:\")\n","print(text_data[:2], end='\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkTN-x8YveQo"},"source":["alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789,.?;:-_()=+@$!&/\\'\\\" ' \n","alphabet_size = len(alphabet)\n","\n","# Define a mapping between characters and indices\n","char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n","int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n","\n","max_text_length = 0\n","for d in text_data:\n","  l = len(d)\n","  if l > max_text_length:\n","    max_text_length = l\n","\n","num_rows = len(text_data)\n","labels = np.empty((num_rows, max_text_length, alphabet_size))\n","\n","for i, d in enumerate(text_data):\n","  # Target text data -> integer encodings\n","  integer_encodings = [char_to_int[char] for char in d]\n","\n","  # -> one-hot encodings\n","  one_hot = tf.one_hot(indices=integer_encodings, depth=alphabet_size)\n","  labels[i] = one_hot.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BM9Hr5-AvknD"},"source":["img_width = 6613\n","class CTCLayer(layers.Layer):\n","    def __init__(self, name=None):\n","        super().__init__(name=name)\n","        self.loss_fn = keras.backend.ctc_batch_cost\n","\n","    def call(self, y_true, y_pred):\n","        # Compute the training-time loss value and add it\n","        # to the layer using `self.add_loss()`.\n","        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n","        print(batch_len)\n","        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n","        print(input_length)\n","        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n","\n","        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n","        self.add_loss(loss)\n","\n","        # At test time, just return the computed predictions\n","        return y_pred\n","\n","def create_model():\n","\n","        labels_new = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n","        input_img = layers.Input(shape=(img_width,1), name=\"image\", dtype=\"float32\")\n","\n","\n","        x = layers.Conv1D(filters=64, kernel_size=7, activation=\"relu\", kernel_initializer=\"he_normal\",padding=\"same\")(input_img)\n","\n","        x = layers.Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding='same')(x)\n","        x = layers.BatchNormalization(trainable=True)(x)\n","\n","        x = layers.Conv1D(filters=256, kernel_size=9,activation=\"relu\", padding='same')(x)\n","        x = layers.BatchNormalization(trainable=True)(x)\n","\n","        x = layers.Conv1D(filters=128, kernel_size=7,activation=\"relu\", padding='same')(x)\n","        x = layers.BatchNormalization(trainable=True) (x)\n","\n","\n","\n","        x = layers.Bidirectional(layers.LSTM(units=128, return_sequences=True, dropout=0.25), name='bi_lstm1')(x)\n","        x = layers.Bidirectional(layers.LSTM(units=64, return_sequences=True, dropout=0.25), name='bi_lstm2') (x)\n","        x = layers.Dense(units=alphabet_size+1, activation = \"softmax\") (x)\n","\n","        output = CTCLayer(name=\"ctc_loss\")(labels_new, x)\n","\n","    # Define the model\n","        model = keras.models.Model(inputs=[input_img, labels_new], outputs=output, name=\"ocr_model_v1\")\n","    # Optimizer\n","        opt = keras.optimizers.Adam()\n","    # Compile the model and return\n","        model.compile(optimizer=opt)\n","\n","\n","\n","        # # loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([x, labels, input_length, label_length])\n","        \n","        # model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\n","        # # model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[metric])\n","\n","        # print(model.summary())\n","        return model "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3EtnFIBvl3O"},"source":["model = create_model()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Psplwt-vpUB"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedKFold\n","# train_data, test_data, train_lbl, test_lbl = train_test_split(data, labels, test_size=0.2, random_state=42)\n","random_state = 42\n","rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=random_state)\n","for train, test in rkf.split(data):\n","    print(\"%s %s\" % (train, test))\n","\n","x_train, x_test, y_train, y_test = data[train], data[test], labels[train], labels[test]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4t8MdAKvsix"},"source":["batch_size = 64\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xc3L18Fxvw2g"},"source":["epochs = 5\n","early_stopping_patience = 5\n","# Add early stopping\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",")\n","\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=epochs,\n","    callbacks=[early_stopping])"],"execution_count":null,"outputs":[]}]}